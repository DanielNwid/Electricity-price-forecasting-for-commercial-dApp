{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c14453b7-d529-4ea0-9eab-8be154048d63",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a995789f-3c8b-4773-8a64-45b7467f0ad6",
   "metadata": {},
   "source": [
    "### Loading Datasets (Cells 11, 19–24);\r\n",
    "Loaeds five datasets (prices, generation, consumption, forecast generation, and cross-border flows) from CSV files using pandas.\n",
    " Specifids semicolon (`;`) as the delimiter and parsds the `Start date` column as datetime.\n",
    " Handlds mixed data types with warnings suppressed using `low_memory=Falsey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b75d053d-2a71-4577-b8b8-4f8805d47a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7352\\3405506440.py:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  prices = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "prices = pd.read_csv(\n",
    "    r\"C:\\Users\\HP\\Documents\\PowercastLunar\\Day-ahead_prices_202301010000_202503050000_Hour.csv\",\n",
    "    delimiter=\";\",  \n",
    "    parse_dates=[\"Start date\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5486f231-6677-49d2-97eb-5f5ded7ba399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7352\\1188790165.py:1: DtypeWarning: Columns (3,7,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  generation = pd.read_csv(\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7352\\1188790165.py:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  generation = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "generation = pd.read_csv(\n",
    "    r\"C:\\Users\\HP\\Documents\\PowercastLunar\\Actual_generation_202301010000_202503050000_Quarterhour.csv\",\n",
    "    delimiter=\";\",\n",
    "    parse_dates=[\"Start date\"]\n",
    ")\n",
    "generation = clean_column_names(generation)\n",
    "# Rename ambiguous columns \n",
    "generation.rename(columns={\"other_conventional_mwh\": \"fossil_other_mwh\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e32fa3f-992c-4208-9399-6de295d73ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7352\\1789502006.py:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  consumption = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "consumption = pd.read_csv(\n",
    "    r\"C:\\Users\\HP\\Documents\\PowercastLunar\\Actual_consumption_202301010000_202503050000_Quarterhour.csv\",\n",
    "    delimiter=\";\",\n",
    "    parse_dates=[\"Start date\"]\n",
    ")\n",
    "consumption = clean_column_names(consumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "279f99d2-3c6d-443c-bdb5-64a646316c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7352\\1803036387.py:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  forecast_gen = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "forecast_gen = pd.read_csv(\n",
    "    r\"C:\\Users\\HP\\Documents\\PowercastLunar\\Forecasted_generation_Day-Ahead_202301010000_202503050000_Hour_Quarterhour.csv\",\n",
    "    delimiter=\";\",\n",
    "    parse_dates=[\"Start date\"]\n",
    ")\n",
    "forecast_gen = clean_column_names(forecast_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a702c337-f4b4-402b-8555-97e3b02a60ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7352\\3964097221.py:1: DtypeWarning: Columns (5,7,8,9,10,11,12,13,14,15,16,17,19,20,21,22,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cross_border = pd.read_csv(\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7352\\3964097221.py:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  cross_border = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "cross_border = pd.read_csv(\n",
    "    r\"C:\\Users\\HP\\Documents\\PowercastLunar\\Cross-border_physical_flows_202301010000_202503050000_Quarterhour.csv\",\n",
    "    delimiter=\";\",\n",
    "    parse_dates=[\"Start date\"]\n",
    ")\n",
    "cross_border = clean_column_names(cross_border)\n",
    "# Rename columns for clarity \n",
    "cross_border.rename(columns={\"net_export_mwh_original_resolutions\": \"net_export_mwh\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c429205a-8eb3-416f-8e55-886922d77d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78139ab2-915d-4d7a-995e-75f67d5827f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c50b21bc-effc-45a5-ac34-b71d26d8b689",
   "metadata": {},
   "source": [
    "### Cleaning Column Names\n",
    "* Standardized column names across datasets by converting to lowercase, replacing special characters (e.g., `[`, `]`, `€`) with underscores. \n",
    "* Removed unnecessary suffixes like \"original resolutions\".\n",
    "* Ensured consistency for merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1221e231-5886-4a46-aea2-caf4ff63a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(df):\n",
    "    \"\"\"Standardize column names (lowercase, underscores, remove units).\"\"\"\n",
    "    df.columns = (\n",
    "        df.columns.str.lower()\n",
    "        .str.replace(r\"[\\[\\]€/]\", \"\", regex=True)\n",
    "        .str.replace(\"original resolutions\", \"\", case=False)\n",
    "        .str.replace(\" \", \"_\")\n",
    "        .str.strip(\"_\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "prices = clean_column_names(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbc7bcf0-87e1-4e8e-9f4a-6c3916b0f919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start_date', 'end_date', 'germanyluxembourg_mwh', '∅_delu_neighbours_mwh', 'belgium_mwh', 'denmark_1_mwh', 'denmark_2_mwh', 'france_mwh', 'netherlands_mwh', 'norway_2_mwh', 'austria_mwh', 'poland_mwh', 'sweden_4_mwh', 'switzerland_mwh', 'czech_republic_mwh', 'deatlu_mwh', 'northern_italy_mwh', 'slovenia_mwh', 'hungary_mwh']\n"
     ]
    }
   ],
   "source": [
    "print(prices.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fad430-1ee1-4f41-b8e2-a792b2a02b73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4101634-7699-461c-ba31-528f6b308ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90014e00-3af5-463f-874a-38a85dc27c11",
   "metadata": {},
   "source": [
    "### Processing Quarter-Hourly Data\r",
    "* Resampleds quarter-hourly datasets (generation, consumption, cross-border) to hourly intervals.\n",
    "*  Convereds `start_date` to datetime with a specific format (`%b %d, %Y %I:%M %p` \n",
    "*  dropeds non-numeric``end_dat` .\n",
    "*  convereds remaining columns to numeric (coercing errors to NaN\n",
    "*  aggregatd  using mean values per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "831e64b0-baa0-4664-8100-b378e9da616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing prices ===\n",
      "Processed prices with columns: ['start_date', 'end_date', 'germanyluxembourg_mwh', 'delu_neighbours_mwh', 'belgium_mwh', 'denmark_1_mwh', 'denmark_2_mwh', 'france_mwh', 'netherlands_mwh', 'norway_2_mwh', 'austria_mwh', 'poland_mwh', 'sweden_4_mwh', 'switzerland_mwh', 'czech_republic_mwh', 'deatlu_mwh', 'northern_italy_mwh', 'slovenia_mwh', 'hungary_mwh']\n",
      "\n",
      "=== Processing generation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7352\\454245991.py:56: DtypeWarning: Columns (3,7,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, delimiter=\";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed generation with columns: ['start_date', 'biomass_mwh', 'hydropower_mwh', 'wind_offshore_mwh', 'wind_onshore_mwh', 'photovoltaics_mwh', 'other_renewable_mwh', 'nuclear_mwh', 'lignite_mwh', 'hard_coal_mwh', 'fossil_gas_mwh', 'hydro_pumped_storage_mwh', 'other_conventional_mwh']\n",
      "\n",
      "=== Processing consumption ===\n",
      "Processed consumption with columns: ['start_date', 'residual_load_mwh', 'hydro_pumped_storage_mwh']\n",
      "\n",
      "=== Processing cross_border ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7352\\454245991.py:56: DtypeWarning: Columns (5,7,8,9,10,11,12,13,14,15,16,17,19,20,21,22,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, delimiter=\";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed cross_border with columns: ['start_date', 'net_export_mwh', 'netherlands_export_mwh', 'netherlands_import_mwh', 'switzerland_export_mwh', 'switzerland_import_mwh', 'denmark_export_mwh', 'denmark_import_mwh', 'czech_republic_export_mwh', 'czech_republic_import_mwh', 'luxembourg_export_mwh', 'luxembourg_import_mwh', 'sweden_export_mwh', 'sweden_import_mwh', 'austria_export_mwh', 'austria_import_mwh', 'france_export_mwh', 'france_import_mwh', 'poland_export_mwh', 'poland_import_mwh', 'norway_export_mwh', 'norway_import_mwh', 'belgium_export_mwh', 'belgium_import_mwh']\n",
      "\n",
      "All datasets processed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_column_names(df):\n",
    "    \"\"\"Standardize column names (lowercase, underscores, remove units/symbols).\"\"\"\n",
    "    df.columns = (\n",
    "        df.columns.str.lower()\n",
    "        .str.replace(r\"[\\[\\]€/∅()]\", \"\", regex=True)\n",
    "        .str.replace(\"original resolutions\", \"\", case=False)\n",
    "        .str.replace(\" \", \"_\")\n",
    "        .str.strip(\"_\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def process_dataset(df):\n",
    "    \"\"\"Process individual dataset with strict type handling.\"\"\"\n",
    "    # Convert dates\n",
    "    df[\"start_date\"] = pd.to_datetime(\n",
    "        df[\"start_date\"],\n",
    "        format=\"%b %d, %Y %I:%M %p\",\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "    df = df.dropna(subset=[\"start_date\"])\n",
    "    \n",
    "    # Remove non-numeric columns \n",
    "    df = df.drop(columns=[\"end_date\"], errors=\"ignore\")\n",
    "    \n",
    "    # Convert all remaining columns to numeric\n",
    "    for col in df.columns:\n",
    "        if col != \"start_date\":\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    \n",
    "    # Drop columns that failed conversion (all NaN)\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    \n",
    "    # Resample\n",
    "    return (\n",
    "        df.set_index(\"start_date\")\n",
    "        .resample(\"H\")\n",
    "        .mean(numeric_only=True)  \n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "# Load and process datasets\n",
    "data_paths = {\n",
    "    \"prices\": r\"C:\\Users\\HP\\Documents\\PowercastLunar\\Day-ahead_prices_202301010000_202503050000_Hour.csv\",\n",
    "    \"generation\": r\"C:\\Users\\HP\\Documents\\PowercastLunar\\Actual_generation_202301010000_202503050000_Quarterhour.csv\",\n",
    "    \"consumption\": r\"C:\\Users\\HP\\Documents\\PowercastLunar\\Actual_consumption_202301010000_202503050000_Quarterhour.csv\",\n",
    "    \"cross_border\": r\"C:\\Users\\HP\\Documents\\PowercastLunar\\Cross-border_physical_flows_202301010000_202503050000_Quarterhour.csv\"\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "for name, path in data_paths.items():\n",
    "    print(f\"\\n=== Processing {name} ===\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(path, delimiter=\";\")\n",
    "    df = clean_column_names(df)\n",
    "    \n",
    "    # Process quarter-hourly data\n",
    "    if name != \"prices\":\n",
    "        df = process_dataset(df)\n",
    "    \n",
    "    datasets[name] = df\n",
    "    print(f\"Processed {name} with columns:\", df.columns.tolist())\n",
    "\n",
    "print(\"\\nAll datasets processed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ac1dd7-89b9-4850-b5a3-79cdad87dbcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5832595-3460-41a9-bae4-7780cee28ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82f0ca76-4a1d-4d83-b1da-6857aac60464",
   "metadata": {},
   "source": [
    "### Merging Datasets\n",
    "* Converted `start_date` in the prices dataset to `datetime64[ns]` to match other datasets. \n",
    "* Merged all datasets (prices, generation, consumption, cross-border) on `start_date` using a left join, preserving all price records. \n",
    "* Results in a unified dataset with 19,056 rows and 56 columns, saved as `cleaned_merged_dataset.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce0c0b8f-caed-487e-82e7-a757034a0875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert prices' start_date to match other datasets\n",
    "datasets[\"prices\"][\"start_date\"] = pd.to_datetime(\n",
    "    datasets[\"prices\"][\"start_date\"],\n",
    "    format=\"%b %d, %Y %I:%M %p\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7d3c2490-65c3-456d-bba4-668800f8545c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prices dtype after conversion: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prices dtype after conversion: {datasets['prices']['start_date'].dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3d9d6ab8-0f6a-42f5-9621-efef0831a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = (\n",
    "    datasets[\"prices\"]\n",
    "    .merge(datasets[\"generation\"], on=\"start_date\", how=\"left\", suffixes=(\"\", \"_gen\"))\n",
    "    .merge(datasets[\"consumption\"], on=\"start_date\", how=\"left\")\n",
    "    .merge(datasets[\"cross_border\"], on=\"start_date\", how=\"left\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a2c2cde6-5540-4938-89c4-c08eb25a3a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data shape: (19056, 56)\n",
      "Columns: ['start_date', 'end_date', 'germanyluxembourg_mwh', 'delu_neighbours_mwh', 'belgium_mwh', 'denmark_1_mwh', 'denmark_2_mwh', 'france_mwh', 'netherlands_mwh', 'norway_2_mwh', 'austria_mwh', 'poland_mwh', 'sweden_4_mwh', 'switzerland_mwh', 'czech_republic_mwh', 'deatlu_mwh', 'northern_italy_mwh', 'slovenia_mwh', 'hungary_mwh', 'biomass_mwh', 'hydropower_mwh', 'wind_offshore_mwh', 'wind_onshore_mwh', 'photovoltaics_mwh', 'other_renewable_mwh', 'nuclear_mwh', 'lignite_mwh', 'hard_coal_mwh', 'fossil_gas_mwh', 'hydro_pumped_storage_mwh_x', 'other_conventional_mwh', 'residual_load_mwh', 'hydro_pumped_storage_mwh_y', 'net_export_mwh', 'netherlands_export_mwh', 'netherlands_import_mwh', 'switzerland_export_mwh', 'switzerland_import_mwh', 'denmark_export_mwh', 'denmark_import_mwh', 'czech_republic_export_mwh', 'czech_republic_import_mwh', 'luxembourg_export_mwh', 'luxembourg_import_mwh', 'sweden_export_mwh', 'sweden_import_mwh', 'austria_export_mwh', 'austria_import_mwh', 'france_export_mwh', 'france_import_mwh', 'poland_export_mwh', 'poland_import_mwh', 'norway_export_mwh', 'norway_import_mwh', 'belgium_export_mwh', 'belgium_import_mwh']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Merged data shape: {merged.shape}\")\n",
    "print(f\"Columns: {merged.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "36e84050-ea7a-465e-8cd5-6c56d9c741f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 'end_date' column.\n",
      "Renamed hydro pumped storage columns.\n",
      "Missing percentages in key columns:\n",
      " germanyluxembourg_mwh     0.000000\n",
      "residual_load_mwh        97.223971\n",
      "net_export_mwh           60.054576\n",
      "dtype: float64\n",
      "Interpolated missing values in key columns.\n",
      "Missing values after handling:\n",
      " germanyluxembourg_mwh     0\n",
      "residual_load_mwh         2\n",
      "net_export_mwh           17\n",
      "dtype: int64\n",
      "Confirmed 'start_date' is datetime.\n",
      "Numerical columns converted to float.\n",
      "Saved cleaned dataset to 'cleaned_merged_dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Drop the 'end_date' column\n",
    "merged = merged.drop(columns=['end_date'])\n",
    "print(\"Dropped 'end_date' column.\")\n",
    "\n",
    "# Rename hydro pumped storage columns\n",
    "merged = merged.rename(columns={\n",
    "    'hydro_pumped_storage_mwh_x': 'hydro_pumped_gen_mwh',\n",
    "    'hydro_pumped_storage_mwh_y': 'hydro_pumped_cons_mwh'\n",
    "})\n",
    "print(\"Renamed hydro pumped storage columns.\")\n",
    "\n",
    "# Check missing values in key columns\n",
    "key_columns = ['germanyluxembourg_mwh', 'residual_load_mwh', 'net_export_mwh']\n",
    "missing_percentages = merged[key_columns].isnull().mean() * 100\n",
    "print(\"Missing percentages in key columns:\\n\", missing_percentages)\n",
    "\n",
    "# Handle missing values\n",
    "if missing_percentages.max() < 5:\n",
    "    merged = merged.dropna(subset=key_columns)\n",
    "    print(\"Dropped rows with missing values in key columns.\")\n",
    "else:\n",
    "    merged[key_columns] = merged[key_columns].interpolate(method='linear')\n",
    "    print(\"Interpolated missing values in key columns.\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(\"Missing values after handling:\\n\", merged[key_columns].isnull().sum())\n",
    "\n",
    "# Ensure 'start_date' is datetime\n",
    "merged['start_date'] = pd.to_datetime(merged['start_date'], errors='coerce')\n",
    "assert pd.api.types.is_datetime64_any_dtype(merged['start_date']), \"start_date is not datetime\"\n",
    "print(\"Confirmed 'start_date' is datetime.\")\n",
    "\n",
    "# Convert numerical columns to float\n",
    "for col in key_columns + ['hydro_pumped_gen_mwh', 'hydro_pumped_cons_mwh']:\n",
    "    merged[col] = pd.to_numeric(merged[col], errors='coerce')\n",
    "print(\"Numerical columns converted to float.\")\n",
    "\n",
    "# Save the cleaned dataset\n",
    "merged.to_csv('cleaned_merged_dataset.csv', index=False)\n",
    "print(\"Saved cleaned dataset to 'cleaned_merged_dataset.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d980961-f4c8-4c7d-822e-17875ba4cd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "47010570-d962-4e6e-8b98-b9489c411363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after final cleaning:\n",
      " germanyluxembourg_mwh    0\n",
      "net_export_mwh           0\n",
      "dtype: int64\n",
      "Updated cleaned dataset saved to 'cleaned_merged_dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Drop 'residual_load_mwh' due to excessive missing data\n",
    "merged = merged.drop(columns=['residual_load_mwh'])\n",
    "\n",
    "# Impute missing values in 'net_export_mwh' with the median\n",
    "median_net_export = merged['net_export_mwh'].median()\n",
    "merged['net_export_mwh'] = merged['net_export_mwh'].fillna(median_net_export)\n",
    "\n",
    "# Drop rows with any remaining missing values in key columns\n",
    "key_columns = ['germanyluxembourg_mwh', 'net_export_mwh']\n",
    "merged = merged.dropna(subset=key_columns)\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(\"Missing values after final cleaning:\\n\", merged[key_columns].isnull().sum())\n",
    "\n",
    "# Save the updated cleaned dataset\n",
    "merged.to_csv('cleaned_merged_dataset.csv', index=False)\n",
    "print(\"Updated cleaned dataset saved to 'cleaned_merged_dataset.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc3f276-cfb7-456b-87bc-ecc35633aebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
